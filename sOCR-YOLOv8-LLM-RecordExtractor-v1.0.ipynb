{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4348c4f4-0357-4c33-8025-0843a2835609",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# !pip install -qqq -U git+https://github.com/huggingface/peft.git\n",
    "# !pip install -qqq -U git+https://github.com/huggingface/accelerate.git\n",
    "# !pip install -qqq bitsandbytes\n",
    "# !pip install scipy\n",
    "# !pip install google-cloud-vision\n",
    "# !pip install --upgrade google-auth\n",
    "# !pip install datasets\n",
    "# !pip install evaluate\n",
    "# !pip install transformers -U\n",
    "# !pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7c52ed9-6ec0-499d-8046-420e0552a24b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Open the ZIP file\n",
    "# with zipfile.ZipFile('/root/NA0219_production_batches/NA0219_IIMI0263_4.zip', 'r') as zip_ref:\n",
    "#     # Get the list of files in the ZIP archive\n",
    "#     file_list = zip_ref.namelist()\n",
    "    \n",
    "#     # Initialize tqdm progress bar\n",
    "#     with tqdm(total=len(file_list), desc=\"Extracting files\") as pbar:\n",
    "#         # Extract each file\n",
    "#         for file in file_list:\n",
    "#             zip_ref.extract(file, '/root/NA0219_production_batches/NA0219_IIMI0263_4')\n",
    "#             # Update the progress bar\n",
    "#             pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93f77f4-8810-4d34-b087-734cb28f88b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from collections import OrderedDict\n",
    "from google.cloud import vision_v1\n",
    "from google.cloud.vision_v1 import types\n",
    "import json\n",
    "import re\n",
    "import logging\n",
    "import time\n",
    "import zipfile\n",
    "import csv\n",
    "import cv2\n",
    "import shutil\n",
    "from pprint import pprint\n",
    "from datasets import load_dataset\n",
    "import bitsandbytes as bnb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from typing import List\n",
    "from datetime import datetime\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import evaluate\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftConfig,\n",
    "    PeftModel,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import gather_object\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "from ultralytics import YOLO\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = ''\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "model = YOLO('/root/NA0219/runs/detect/train2/weights/last.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e780851-5cd0-4e43-87b7-36554984bc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  Loading config file\n",
    "# def load_config(file_path):\n",
    "#     with open(file_path, 'r') as file:\n",
    "#         config = json.load(file)\n",
    "#     return config\n",
    "\n",
    "# json_file_path = '/root/NA0219/config_219.json'\n",
    "\n",
    "# configurations = load_config(json_file_path)\n",
    "\n",
    "# compressed_file_path = configurations['zip_file_path']\n",
    "# folder_to_process_path = configurations['directory_path']\n",
    "# output_txt_path = os.path.splitext(os.path.basename(folder_to_process_path))[0]\n",
    "\n",
    "# print(\"Compressed File Path:\", compressed_file_path)\n",
    "# print(\"Folder to process:\", folder_to_process_path)\n",
    "\n",
    "# # Deleting the extra file after unzipping\n",
    "# with zipfile.ZipFile(compressed_file_path, 'r') as zip_ref: zip_ref.extractall(folder_to_process_path)\n",
    "\n",
    "# # If there are subfolders, move their contents to the base folder and delete the subfolders\n",
    "# subfolders = [f for f in os.listdir(folder_to_process_path) if os.path.isdir(os.path.join(folder_to_process_path, f))]\n",
    "# for subfolder in subfolders:\n",
    "#     subfolder_path = os.path.join(folder_to_process_path, subfolder)\n",
    "#     for item in os.listdir(subfolder_path):\n",
    "#         item_path = os.path.join(subfolder_path, item)\n",
    "#         new_item_path = os.path.join(folder_to_process_path, item)\n",
    "#         shutil.move(item_path, new_item_path)\n",
    "#     # Remove the now-empty subfolder\n",
    "#     os.rmdir(subfolder_path)\n",
    "\n",
    "# # # move folder\n",
    "# # #shutil.move(folder_to_process_path, \"/root/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dde39e3-8a62-479b-9243-449a6d1611f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c35cae4ef020453cbad4555705b9452d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model_id = \"HuggingFaceH4/zephyr-7b-alpha\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    add_bos_token=True,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    model_max_length=4096,\n",
    "    padding=True,\n",
    "    add_eos_token=True)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "ft_model = PeftModel.from_pretrained(base_model, \"/root/NA0219/zephyr_7B_alpha-finetune-22422_NA0219_9_sep_2024/checkpoint-22000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3ef8a9c-0b0e-447b-acfe-5fff5680bde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def google_cloud_vision_response(img):\n",
    "    try:\n",
    "        success, encoded_image = cv2.imencode('.jpg', img)\n",
    "        if not success:\n",
    "            raise ValueError(\"Failed to encode image\")\n",
    "\n",
    "        content = encoded_image.tobytes()\n",
    "        imgocr = vision_v1.Image(content=content)\n",
    "        image_context = vision_v1.ImageContext(language_hints=['nl', 'de'])\n",
    "\n",
    "        response = vision_v1.ImageAnnotatorClient().document_text_detection(image=imgocr, image_context=image_context)\n",
    "        if not response:\n",
    "            raise ValueError(\"No response from Google Cloud Vision\")\n",
    "\n",
    "        return response\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error in Google Cloud Vision API:\", e)\n",
    "        return None\n",
    "\n",
    "bounding_boxes_info = []\n",
    "\n",
    "def detect_crop_and_order_objects(img_path):\n",
    "    # Step 1: Detect objects and get bounding boxes\n",
    "    img = cv2.imread(img_path)\n",
    "    detect_result = model(img)\n",
    "    bboxes = []\n",
    "    \n",
    "    if detect_result[0].boxes.data.shape[0] > 0:\n",
    "        for box in detect_result[0].boxes.data.tolist():\n",
    "            xmin, ymin, xmax, ymax, conf, cls = box\n",
    "            bboxes.append([img_path, cls, int(xmin), int(ymin), int(xmax), int(ymax), conf])\n",
    "    \n",
    "    # Step 2: Order the detections sequentially\n",
    "    img_height, img_width = img.shape[:2]\n",
    "    \n",
    "    # Find the midpoint to split left and right sides\n",
    "    midpoint = img_width / 2\n",
    "    \n",
    "    # Separate bounding boxes into left and right sides based on x-coordinates\n",
    "    left_records = [bbox for bbox in bboxes if bbox[2] + bbox[4] / 2 < midpoint]  # center of bbox for left side\n",
    "    right_records = [bbox for bbox in bboxes if bbox[2] + bbox[4] / 2 >= midpoint]  # center of bbox for right side\n",
    "    \n",
    "    # Sort left side (top-to-bottom based on ymin)\n",
    "    left_records_sorted = sorted(left_records, key=lambda x: x[3])  # ymin for sorting\n",
    "    \n",
    "    # Sort right side (top-to-bottom based on ymin)\n",
    "    right_records_sorted = sorted(right_records, key=lambda x: x[3])  # ymin for sorting\n",
    "    \n",
    "    # Merge both sequences: left side first, then right side\n",
    "    ordered_bboxes = left_records_sorted + right_records_sorted\n",
    "    \n",
    "    # Extract coordinates\n",
    "    coordinates = [(bbox[2], bbox[3], bbox[4], bbox[5]) for bbox in ordered_bboxes]  # xmin, ymin, xmax, ymax\n",
    "    \n",
    "    # Update global list with image name and ordered coordinates\n",
    "    bounding_boxes_info.append({\n",
    "        'Image': img_path,\n",
    "        'coordinates': coordinates\n",
    "    })\n",
    "    \n",
    "    return ordered_bboxes\n",
    "\n",
    "def get_response_inside_roi(response, bboxes):\n",
    "    words = []\n",
    "    confidences = []\n",
    "    bounding_boxes = []\n",
    "    image_names = []  # List to store image names\n",
    "    for bbox in bboxes:\n",
    "        img_path, cls, xmin, ymin, xmax, ymax, conf = bbox\n",
    "        image_names.append(os.path.basename(img_path))  # Extract image name from the image path\n",
    "\n",
    "        for page in response.full_text_annotation.pages:\n",
    "            for block in page.blocks:\n",
    "                for paragraph in block.paragraphs:\n",
    "                    for word in paragraph.words:\n",
    "                        word_text = ''.join([symbol.text for symbol in word.symbols])\n",
    "                        x1 = word.bounding_box.vertices[0].x\n",
    "                        y1 = word.bounding_box.vertices[0].y\n",
    "                        x2 = word.bounding_box.vertices[2].x\n",
    "                        y2 = word.bounding_box.vertices[2].y\n",
    "\n",
    "                        word_text = ''.join([symbol.text for symbol in word.symbols])\n",
    "                        confidences.append((word_text, word.confidence))\n",
    "                        vertices = [(v.x, v.y) for v in word.bounding_box.vertices]\n",
    "                        bounding_boxes.append((word_text, vertices))\n",
    "\n",
    "                        if xmin <= x1 <= xmax and ymin <= y1 <= ymax and xmin <= x2 <= xmax and ymin <= y2 <= ymax:\n",
    "                            words.append(word_text)\n",
    "\n",
    "    if words:\n",
    "        combined_words = ' '.join(words)\n",
    "        return image_names, combined_words, confidences, bounding_boxes\n",
    "\n",
    "    return None\n",
    "\n",
    "ocrs = []\n",
    "\n",
    "def process_image(img_path, bboxes, ft_model, tokenizer):\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        print(f\"Failed to read the image {img_path}.\")\n",
    "        # return output_format\n",
    "\n",
    "    response = google_cloud_vision_response(img)\n",
    "    processed_image_names = {}\n",
    "\n",
    "    if response is not None:\n",
    "        for bbox in bboxes:\n",
    "            response_inside_roi = get_response_inside_roi(response, [bbox])\n",
    "            if response_inside_roi:\n",
    "                image_name, ocr_data, confidences, bounding_boxes = response_inside_roi\n",
    "\n",
    "                # Handle duplicate image names\n",
    "                base_image_name = image_name[0].rsplit('.', 1)[0]\n",
    "                part_number = processed_image_names.get(base_image_name, 0) + 1\n",
    "                unique_image_name = f\"{base_image_name}_part{part_number}.{image_name[0].rsplit('.', 1)[1]}\"\n",
    "                processed_image_names[base_image_name] = part_number\n",
    "\n",
    "                ocr_texts = [ocr_data]\n",
    "                \n",
    "                ocrs.append({\n",
    "                    'Image': unique_image_name,\n",
    "                    'OCR': ocr_data,\n",
    "                    'Confidence Scores': confidences,\n",
    "                    'Bounding Boxes': bounding_boxes\n",
    "                })\n",
    "    else:\n",
    "        print(f\"No response from Google Cloud Vision for {img_path}\")\n",
    "\n",
    "    return ocrs\n",
    "\n",
    "def process_image_folder(input_folder, max_workers):\n",
    "    images = [f for f in os.listdir(input_folder) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    output_format = []\n",
    "\n",
    "    # img_bboxes = {img: detect_and_crop_objects(os.path.join(input_folder, img)) for img in images}\n",
    "    img_bboxes = {img: detect_crop_and_order_objects(os.path.join(input_folder, img)) for img in images}\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor, tqdm(total=len(images), desc='Processing Images') as pbar:\n",
    "        futures = []\n",
    "        for img_name in images:\n",
    "            img_path = os.path.join(input_folder, img_name)\n",
    "            future = executor.submit(process_image, img_path, img_bboxes[img_name], ft_model, tokenizer)\n",
    "            future.add_done_callback(lambda p: pbar.update())\n",
    "            futures.append(future)\n",
    "\n",
    "        for future in futures:\n",
    "            output_format.extend(future.result())\n",
    "\n",
    "    return output_format\n",
    "\n",
    "def get_image_ocr_list(dict_list):\n",
    "    image_ocr_list = []\n",
    "    for d in dict_list:\n",
    "        if 'OCR' in d and 'Image' in d:\n",
    "            image_ocr_dict = {d['Image']: d['OCR']}\n",
    "            image_ocr_list.append(image_ocr_dict)\n",
    "        elif 'Image' in d:\n",
    "            image_ocr_dict = {d['Image']: None}\n",
    "            image_ocr_list.append(image_ocr_dict)\n",
    "    return image_ocr_list\n",
    "\n",
    "\n",
    "all_outputs = []\n",
    "\n",
    "def process_ocr_data(ocr_data, ft_model, tokenizer, batch_size=20, n_batches_before_error=500):\n",
    "    batches_processed = 0\n",
    "\n",
    "    try:\n",
    "        ft_model.eval()\n",
    "\n",
    "        print(f\"Total OCR entries: {len(ocr_data)}\")\n",
    "\n",
    "        for i in tqdm(range(0, len(ocr_data), batch_size), desc=\"Processing batches\", unit=\"batch\"):\n",
    "            while True:\n",
    "                try:\n",
    "                    batch_data = ocr_data[i:i + batch_size]\n",
    "\n",
    "                    print(f\"\\nProcessing batch from index {i} to {i + batch_size}\")\n",
    "\n",
    "                    batch_images = [list(d.keys())[0] for d in batch_data]\n",
    "                    batch_texts = [list(d.values())[0] for d in batch_data]\n",
    "\n",
    "                    eval_prompts = [\n",
    "                        f\"\"\"Extract specific data from the provided OCR text of a marriage record. Identify and return the following details in a structured JSON format: Application number, date of marriage ( month,day, year), place of marriage, birth details (month,day, year, place) of both spouses, their county, gender, given names, residence, state, and surnames. Only show the extracted information. Don't show the OCR Text again. Also don't show the pattern which you are asked to follow. Concentrate on the data asked to extract . The output must strictly adhere to the given details, starting with an opening curly brace '{{' and ending with a closing curly brace '}}'. Ensure that each key-value pair contains only accurate and relevant information from the OCR text. Avoid including any extraneous or irrelevant data that does not correspond directly to the specified keys. The structure of the output is as follows:\n",
    "\n",
    "Event_Day_orig: The original day of the event.\n",
    "Event_Month_orig: The original month of the event.\n",
    "Event_Year_orig: The original year of the event.\n",
    "Given_Name_orig: The given name of the person.\n",
    "Surname_orig: The surname of the person.\n",
    "Sex_orig: The gender of the person.\n",
    "Age_orig: The age of the person.\n",
    "Birth_Year_orig: The original year of birth of the person.\n",
    "Death_Year_orig: The original year of death of the person.\n",
    "Father_Given_Name_orig: The given name of the person's father.\n",
    "Father_Surname_orig: The surname of the person's father.\n",
    "Mother_Given_Name_orig: The given name of the person's mother.\n",
    "Mother_Surname_orig: The surname of the person's mother.\n",
    "Spouse_Given_Name_orig: The given name of the spouse.\n",
    "Spouse_Surname_orig: The surname of the spouse.\n",
    "Spouse_Age_orig: The age of the spouse.\n",
    "Spouse_Birth_Year_orig: The original year of birth of the spouse.\n",
    "Spouse_Father_Given_Name_orig: The given name of the spouse's father.\n",
    "Spouse_Father_Surname_orig: The surname of the spouse's father.\n",
    "Spouse_Mother_Given_Name_orig: The given name of the spouse's mother.\n",
    "Spouse_Mother_Surname_orig: The surname of the spouse's mother.\n",
    "\n",
    "### OCR Text:\n",
    "{text}\n",
    "\"\"\" for text in batch_texts]\n",
    "\n",
    "                    batch_inputs = tokenizer(eval_prompts, padding=True, return_tensors=\"pt\").to(ft_model.device)\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        batch_outputs = ft_model.generate(**batch_inputs, max_new_tokens=1000)\n",
    "                        decoded_outputs = tokenizer.batch_decode(batch_outputs, skip_special_tokens=True)\n",
    "\n",
    "                    batch_results = [{'Image': img, 'Model_Output': output} for img, output in zip(batch_images, decoded_outputs)]\n",
    "                    all_outputs.extend(batch_results)\n",
    "\n",
    "                    batches_processed += 1\n",
    "\n",
    "                    if batches_processed >= n_batches_before_error:\n",
    "                        raise torch.cuda.OutOfMemoryError(\"Intentionally triggered CUDA out of memory error after 500 batches.\")\n",
    "\n",
    "                    break\n",
    "\n",
    "                except torch.cuda.OutOfMemoryError:\n",
    "                    print(\"Threshold reached. Batch terminated\")\n",
    "                    raise\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Kernel interrupted. Saving current outputs and exiting.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    return all_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82bb1c57-1312-4a3e-bd63-c2d85acbbf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_info =[]\n",
    "def split_bbox_info(bbox_info):\n",
    "    for entry in bbox_info:\n",
    "        # Extract the base name of the image without the extension\n",
    "        img_base_name = os.path.splitext(os.path.basename(entry['Image']))[0]\n",
    "        \n",
    "        # Loop through each set of coordinates and create a new entry\n",
    "        for i, coords in enumerate(entry['coordinates'], start=1):\n",
    "            split_info.append({\n",
    "                'Image': f'{img_base_name}_part{i}.jpg',\n",
    "                'coordinates': coords\n",
    "            })\n",
    "    \n",
    "    return split_info\n",
    "def is_within_coordinates(bbox, coordinates):\n",
    "    x_min, y_min, x_max, y_max = coordinates\n",
    "    x1, y1 = bbox[0]\n",
    "    x2, y2 = bbox[1]\n",
    "    x3, y3 = bbox[2]\n",
    "    x4, y4 = bbox[3]\n",
    "    return (x_min <= x1 <= x_max and y_min <= y1 <= y_max and\n",
    "            x_min <= x2 <= x_max and y_min <= y2 <= y_max and\n",
    "            x_min <= x3 <= x_max and y_min <= y3 <= y_max and\n",
    "            x_min <= x4 <= x_max and y_min <= y4 <= y_max)\n",
    "\n",
    "def extract_filtered_info_v2(data_list, split_info):\n",
    "    filtered_ocrs = []\n",
    "\n",
    "    # Create a dictionary to map images to their coordinates\n",
    "    coordinates_map = {entry['Image']: entry['coordinates'] for entry in split_info}\n",
    "\n",
    "    for data in data_list:\n",
    "        if not isinstance(data, dict):\n",
    "            raise TypeError(\"Each item in the input data must be a dictionary\")\n",
    "\n",
    "        image_name = data.get('Image', '')\n",
    "        if image_name not in coordinates_map:\n",
    "            print(f\"Image '{image_name}' not found in split_info\")\n",
    "            continue\n",
    "        \n",
    "        ocr_text = data.get('OCR', '')\n",
    "        confidence_scores_list = data.get('Confidence Scores', [])\n",
    "        bounding_boxes_list = data.get('Bounding Boxes', [])\n",
    "\n",
    "        confidence_dict = {word: score for word, score in confidence_scores_list}\n",
    "        bounding_boxes_dict = {word: box for word, box in bounding_boxes_list}\n",
    "\n",
    "        filtered_confidence_scores = []\n",
    "        filtered_bounding_boxes = []\n",
    "        filtered_ocr = []\n",
    "\n",
    "        coords = coordinates_map[image_name]\n",
    "\n",
    "        for word in ocr_text.split():\n",
    "            if word in confidence_dict and word in bounding_boxes_dict:\n",
    "                bbox = bounding_boxes_dict[word]\n",
    "\n",
    "                if is_within_coordinates(bbox, coords):\n",
    "                    filtered_confidence_scores.append((word, confidence_dict[word]))\n",
    "                    filtered_bounding_boxes.append((word, bbox))\n",
    "                    filtered_ocr.append(word)\n",
    "\n",
    "        filtered_data = {\n",
    "            'Image': image_name,\n",
    "            'Coordinates': coords,\n",
    "            'OCR': ' '.join(filtered_ocr),\n",
    "            'Confidence Scores': filtered_confidence_scores,\n",
    "            'Bounding Boxes': filtered_bounding_boxes\n",
    "        }\n",
    "        filtered_ocrs.append(filtered_data)\n",
    "\n",
    "    return filtered_ocrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf045140-92c8-4dd5-860c-1e42cf6daa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_lists(l1, l2):\n",
    "    final_results = []\n",
    "    \n",
    "    for dict1 in l1:\n",
    "        for dict2 in l2:\n",
    "            if dict1['Image'] == dict2['Image']:\n",
    "                merged_dict = dict2.copy()\n",
    "                merged_dict.update(dict1)\n",
    "                final_results.append(merged_dict)\n",
    "                break\n",
    "    \n",
    "    return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95e50435-25cb-4320-9cf9-e42544d5ddc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json_outputs_parallel(final_results, max_workers=8):\n",
    "    \"\"\"\n",
    "    Parallel function to extract and modify JSON outputs from the results.\n",
    "    :param final_results: List of dictionaries, each being an output from the previous function.\n",
    "    :param max_workers: Maximum number of threads to use for parallel processing.\n",
    "    :return: List of modified JSON outputs.\n",
    "    \"\"\"\n",
    "    def process_item(item):\n",
    "        # Accessing the string within the list for Model Output\n",
    "        # inference_text = item['Model_output'][0]\n",
    "        inference_text = item['Model_Output']\n",
    "        \n",
    "        pattern_1 = r\"\"\"\n",
    "        Event_Day_orig:\\s*(?P<Event_Day>\\d*)\\n\n",
    "        Event_Month_orig:\\s*(?P<Event_Month>[A-Za-z]*)\\n\n",
    "        Event_Year_orig:\\s*(?P<Event_Year>\\d*)\\n\n",
    "        Given_Name_orig:\\s*(?P<Given_Name>[\\w\\s]*)\\n\n",
    "        Surname_orig:\\s*(?P<Surname>[\\w\\s]*)\\n\n",
    "        Sex_orig:\\s*(?P<Sex>\\w*)\\n\n",
    "        Age_orig:\\s*(?P<Age>\\d*)\\n\n",
    "        Birth_Year_orig:\\s*(?P<Birth_Year>\\d*)\\n\n",
    "        Death_Year_orig:\\s*(?P<Death_Year>\\d*)\\n\n",
    "        Father_Given_Name_orig:\\s*(?P<Father_Given_Name>[\\w\\s]*)\\n\n",
    "        Father_Surname_orig:\\s*(?P<Father_Surname>[\\w\\s]*)\\n\n",
    "        Mother_Given_Name_orig:\\s*(?P<Mother_Given_Name>[\\w\\s]*)\\n\n",
    "        Mother_Surname_orig:\\s*(?P<Mother_Surname>[\\w\\s]*)\\n\n",
    "        Spouse_Given_Name_orig:\\s*(?P<Spouse_Given_Name>[\\w\\s]*)\\n\n",
    "        Spouse_Surname_orig:\\s*(?P<Spouse_Surname>[\\w\\s]*)\\n\n",
    "        Spouse_Age_orig:\\s*(?P<Spouse_Age>\\d*)\\n\n",
    "        Spouse_Birth_Year_orig:\\s*(?P<Spouse_Birth_Year>\\d*)\\n\n",
    "        Spouse_Father_Given_Name_orig:\\s*(?P<Spouse_Father_Given_Name>[\\w\\s]*)\\n\n",
    "        Spouse_Father_Surname_orig:\\s*(?P<Spouse_Father_Surname>[\\w\\s]*)\\n\n",
    "        Spouse_Mother_Given_Name_orig:\\s*(?P<Spouse_Mother_Given_Name>[\\w\\s]*)\\n\n",
    "        Spouse_Mother_Surname_orig:\\s*(?P<Spouse_Mother_Surname>[\\w\\s]*)\\n\n",
    "        \"\"\"\n",
    "        \n",
    "        pattern_2 = r'### JSON Output:[\\s\\S]*?({[\\s\\S]*?})'\n",
    "        \n",
    "        # Compile the patterns with the verbose flag for readability\n",
    "        regex_1 = re.compile(pattern_1, re.VERBOSE)\n",
    "        regex_2 = re.compile(pattern_2)\n",
    "        \n",
    "        # Match both patterns against the inference text\n",
    "        match_1 = regex_1.search(inference_text)\n",
    "        match_2 = regex_2.search(inference_text)\n",
    "        \n",
    "        # If match_1 is found, extract and process the JSON data using pattern_1\n",
    "        if match_1:\n",
    "            # Extract the matched data as a dictionary\n",
    "            json_data = match_1.groupdict()\n",
    "            \n",
    "            # Add Image information\n",
    "            json_data['Image'] = item['Image']\n",
    "            \n",
    "            return json_data\n",
    "        # If match_2 is found, extract and process the JSON data using pattern_2\n",
    "        elif match_2:\n",
    "            # Extract the JSON string\n",
    "            json_string = match_2.group(1)\n",
    "            \n",
    "            # Replace single quotes with double quotes for property names\n",
    "            json_string = json_string.replace(\"'\", '\"')\n",
    "            \n",
    "            try:\n",
    "                # Load JSON data\n",
    "                json_data = json.loads(json_string)\n",
    "                \n",
    "                # Add Image information\n",
    "                json_data['Image'] = item['Image']\n",
    "                \n",
    "                return json_data\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON for image {item['Image']}: {e}\")\n",
    "                return None\n",
    "        else:\n",
    "            # Neither pattern matched, return None\n",
    "            return None\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Process items in parallel and retrieve the results\n",
    "        json_outputs = list(filter(None, executor.map(process_item, final_results)))\n",
    "    \n",
    "    return json_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c02fa6b-c73e-4ccb-a905-eaab6050e72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_image_base(image):\n",
    "    # Extract the base part of the image name, excluding any part numbers\n",
    "    match = re.match(r'(.+)_part\\d+\\.jpg$', image)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return image\n",
    "\n",
    "def convert_to_int_if_possible(value):\n",
    "    try:\n",
    "        # Convert to float first, then to int if no decimal part remains\n",
    "        float_value = float(value)\n",
    "        if float_value.is_integer():\n",
    "            return int(float_value)\n",
    "        return float_value\n",
    "    except ValueError:\n",
    "        return value\n",
    "\n",
    "def process_fields(events):\n",
    "    for event in events:\n",
    "        for key, value in event.items():\n",
    "            # Apply conversion only to fields that can be numeric\n",
    "            if isinstance(value, str):\n",
    "                # Strip trailing decimals from numeric strings\n",
    "                event[key] = convert_to_int_if_possible(value)\n",
    "            elif isinstance(value, (int, float)):\n",
    "                # Convert numeric types directly\n",
    "                event[key] = convert_to_int_if_possible(value)\n",
    "\n",
    "def group_events_by_image(events):\n",
    "    # Extract base identifiers for sorting and grouping\n",
    "    for event in events:\n",
    "        event['Image_Base'] = extract_image_base(event['Image'])\n",
    "    \n",
    "    # Process fields to remove extra decimal places\n",
    "    process_fields(events)\n",
    "    \n",
    "    # Sort events by the 'Image_Base' to ensure groups are contiguous\n",
    "    events.sort(key=itemgetter('Image_Base'))\n",
    "    \n",
    "    # Group events by 'Image_Base'\n",
    "    grouped_events = [list(group) for key, group in groupby(events, key=itemgetter('Image_Base'))]\n",
    "    \n",
    "    # Flatten the list of groups into a single list, removing 'Image_Base'\n",
    "    sorted_events = []\n",
    "    for group in grouped_events:\n",
    "        for event in group:\n",
    "            del event['Image_Base']\n",
    "        sorted_events.extend(group)\n",
    "    \n",
    "    return sorted_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b23a1cc3-edd6-4013-ad80-d58cc72e6f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicate_keys(input_list):\n",
    "    output_list = []\n",
    "\n",
    "    for item in input_list:\n",
    "        new_item = {}\n",
    "        for key, value in item.items():\n",
    "            if '_orig' in key and key != 'Image_orig':\n",
    "                new_key = f\"{key.replace('_orig', '')}_hwr\"\n",
    "                new_item[key] = value\n",
    "                new_item[new_key] = value\n",
    "            else:\n",
    "                new_item[key] = value\n",
    "                \n",
    "        output_list.append(new_item)\n",
    "\n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f99f26e1-5950-493d-bf7a-f4c68f010fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confidence_and_prect_direct(key_value, confidence_data, bounding_box_data):\n",
    "    try:\n",
    "        words = key_value.split() if isinstance(key_value, str) else key_value.get('text', '').split()\n",
    "        confidences = []\n",
    "        bounding_boxes = []  # This will store bounding boxes for individual words\n",
    "\n",
    "        for word in words:\n",
    "            # Find and append the confidence score\n",
    "            for confidence_word, confidence in confidence_data:\n",
    "                if word.lower() == confidence_word.lower():  # Use case-insensitive comparison\n",
    "                    confidences.append(confidence)\n",
    "                    break\n",
    "\n",
    "            # Find and append the bounding box\n",
    "            for bbox_word, bbox in bounding_box_data:\n",
    "                if word.lower() == bbox_word.lower():\n",
    "                    # Assume bbox is in the format of (top_left_x, top_left_y, bottom_right_x, bottom_right_y)\n",
    "                    # and that it belongs to the correct part of the image.\n",
    "                    bounding_boxes.append((bbox[0][0], bbox[0][1], bbox[2][0], bbox[2][1]))\n",
    "                    break  # Assuming the first match is used, similar to the confidence lookup\n",
    "\n",
    "        # If there are no bounding boxes, return default values\n",
    "        if not bounding_boxes:\n",
    "            return '0', 0, 0, 0, 0  # Return default values for confidence and bounding box\n",
    "\n",
    "        # Calculate average confidence\n",
    "        average_confidence = str((sum(confidences) / len(confidences))) if confidences else '0'\n",
    "\n",
    "        # Use the bounding box of the first word as the representative, if multiple words\n",
    "        top_left_x, top_left_y, bottom_right_x, bottom_right_y = bounding_boxes[0]\n",
    "\n",
    "        return average_confidence, top_left_x, top_left_y, bottom_right_x, bottom_right_y\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred in get_confidence_and_prect_direct: {str(e)}\")\n",
    "        return '0', 0, 0, 0, 0\n",
    "\n",
    "\n",
    "\n",
    "def process_json_outputs(json_outputs, ocr_data, folder_name):\n",
    "    try:\n",
    "        processed_data = []\n",
    "        batch_name = os.path.basename(folder_name)\n",
    "        for json_output in json_outputs:\n",
    "            modified_output = {\n",
    "                'ImageType': 'Marriage',\n",
    "                'Batch': batch_name,\n",
    "                'Image': json_output.get('Image', ''),\n",
    "                'ZoneUserID': '',\n",
    "                'ZoneDate': '',\n",
    "                'ImageRotation': '',\n",
    "                'Line_Number': '',\n",
    "                'State_File_orig': '',\n",
    "                'State_File_hwr': '',\n",
    "                'State_File_conf': '',\n",
    "                'State_File_PRect': ''\n",
    "            }\n",
    "            image_name = json_output.get('Image', '')\n",
    "            corresponding_ocr_result = next((item for item in ocr_data if item.get('Image', '') == image_name), None)\n",
    "            confidence_data = corresponding_ocr_result.get('Confidence Scores', []) if corresponding_ocr_result else []\n",
    "            bounding_box_data = corresponding_ocr_result.get('Bounding Boxes', []) if corresponding_ocr_result else []\n",
    "\n",
    "            # Separate keys with '_orig' or '_hwr' from other keys\n",
    "            orig_hwr_keys = [key for key in json_output.keys() if key.endswith('_orig') or key.endswith('_hwr')]\n",
    "            other_keys = [key for key in json_output.keys() if key not in orig_hwr_keys]\n",
    "\n",
    "            # Process keys with '_orig' or '_hwr' first\n",
    "            for key in orig_hwr_keys:\n",
    "                base_key = key.rsplit('_', 1)[0]  # Remove suffix like '_hwr' or '_orig'\n",
    "                value = json_output[key]\n",
    "                modified_output[key] = value\n",
    "                if corresponding_ocr_result:\n",
    "                    try:\n",
    "                        confidence, x1_min, y1_min, x2_max, y2_max = get_confidence_and_prect_direct(value, confidence_data, bounding_box_data)\n",
    "                        modified_output[f'{base_key}_conf'] = confidence\n",
    "                        modified_output[f'{base_key}_PRect'] = str(x1_min) + ',' + str(y1_min) + ',' + str(x2_max) + ',' + str(y2_max)\n",
    "                    except Exception as e:\n",
    "                        print(f\"An error occurred while processing key '{key}' for image '{image_name}': {str(e)}\")\n",
    "\n",
    "            # Process other keys\n",
    "            for key in other_keys:\n",
    "                base_key = key.rsplit('_', 1)[0]  # Remove suffix like '_hwr' or '_orig'\n",
    "                value = json_output[key]\n",
    "                modified_output[key] = value\n",
    "                if corresponding_ocr_result and (key.endswith('_orig') or key.endswith('_hwr')):\n",
    "                    try:\n",
    "                        confidence, x1_min, y1_min, x2_max, y2_max = get_confidence_and_prect_direct(value, confidence_data, bounding_box_data)\n",
    "                        modified_output[f'{base_key}_conf'] = confidence\n",
    "                        modified_output[f'{base_key}_PRect'] = str(x1_min) + ',' + str(y1_min) + ',' + str(x2_max) + ',' + str(y2_max)\n",
    "                    except Exception as e:\n",
    "                        print(f\"An error occurred while processing key '{key}' for image '{image_name}': {str(e)}\")\n",
    "\n",
    "            processed_data.append(modified_output)\n",
    "        return processed_data\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred in process_json_outputs: {str(e)}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e403ce55-7108-452a-93f2-fb7c4191a743",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data, split_info):\n",
    "    # Create a mapping of images to their coordinates\n",
    "    split_info_dict = {info['Image']: info['coordinates'] for info in split_info}\n",
    "    \n",
    "    for entry in data:\n",
    "        # Get the image name from the entry\n",
    "        image_name = entry.get('Image')\n",
    "        \n",
    "        # Find the coordinates if the image name is in split_info_dict\n",
    "        if image_name in split_info_dict:\n",
    "            coordinates = split_info_dict[image_name]\n",
    "            coordinates_str = ','.join(map(str, coordinates))\n",
    "        \n",
    "            # Iterate through keys and clean _PRect and _conf fields\n",
    "            for key in list(entry.keys()):\n",
    "                if key.endswith('_PRect'):\n",
    "                    if entry[key] == '0,0,0,0':\n",
    "                        entry[key] = coordinates_str\n",
    "                elif key.endswith('_conf'):\n",
    "                    if entry[key] == 0:\n",
    "                        entry[key] = ' '\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ff29e0c-739a-4868-8229-459b1236d933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_data(input_data):\n",
    "    organized_data = []\n",
    "\n",
    "    for entry in input_data:\n",
    "        new_entry = {}\n",
    "        for key, value in entry.items():\n",
    "            # Skip 'Image_conf' and 'Image_PRect'\n",
    "            if key in ['Image_conf', 'Image_PRect']:\n",
    "                continue\n",
    "\n",
    "            if key.endswith('_orig'):\n",
    "                new_key_hwr = key.replace('_orig', '_hwr')\n",
    "                new_entry[key] = value\n",
    "                new_entry[new_key_hwr] = entry.get(new_key_hwr, '')\n",
    "            elif key.endswith('_hwr'):\n",
    "                continue\n",
    "            else:\n",
    "                new_entry[key] = value\n",
    "\n",
    "        organized_data.append(new_entry)\n",
    "\n",
    "    return organized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d06be352-922f-4a19-908f-e8d89fd09eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_image_names(dict_list):\n",
    "    for item in dict_list:\n",
    "        if 'Image' in item:\n",
    "            # Split the image name by underscore and take the first two parts\n",
    "            parts = item['Image'].split('_')\n",
    "            if len(parts) > 1:\n",
    "                # Combine the first two parts to get the new image name\n",
    "                new_image_name = f\"{parts[0]}_{parts[1]}\"\n",
    "                item['Image'] = new_image_name\n",
    "    return dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd6d0932-1bed-445c-92b6-59c845fb00d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_pipe_delimited_file(list_of_dicts, filename=None):\n",
    "    if not list_of_dicts:\n",
    "        return \"\"\n",
    "    headers = list(list_of_dicts[0].keys())\n",
    "    header_line = '|'.join(headers)\n",
    "    data_lines = []\n",
    "    for entry in list_of_dicts:\n",
    "        values = [str(entry.get(key, '')) for key in headers]\n",
    "        data_lines.append('|'.join(values))\n",
    "    result = f\"{header_line}\\n\"\n",
    "    result += '\\n'.join(data_lines)\n",
    "    if filename:\n",
    "        with open(filename, 'w', encoding='utf-8') as file:\n",
    "            file.write(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "581be86e-3771-4a71-aadc-89007918fa11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 448x640 4 recordss, 79.4ms\n",
      "Speed: 8.9ms preprocess, 79.4ms inference, 775.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 4 recordss, 76.4ms\n",
      "Speed: 3.4ms preprocess, 76.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 recordss, 9.7ms\n",
      "Speed: 2.9ms preprocess, 9.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 recordss, 9.5ms\n",
      "Speed: 2.6ms preprocess, 9.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 recordss, 9.4ms\n",
      "Speed: 2.5ms preprocess, 9.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 recordss, 9.6ms\n",
      "Speed: 2.6ms preprocess, 9.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 recordss, 9.6ms\n",
      "Speed: 2.5ms preprocess, 9.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 recordss, 9.7ms\n",
      "Speed: 2.7ms preprocess, 9.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 recordss, 9.7ms\n",
      "Speed: 2.6ms preprocess, 9.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 recordss, 9.8ms\n",
      "Speed: 2.5ms preprocess, 9.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 recordss, 9.7ms\n",
      "Speed: 2.6ms preprocess, 9.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 recordss, 9.7ms\n",
      "Speed: 2.7ms preprocess, 9.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 recordss, 9.5ms\n",
      "Speed: 2.5ms preprocess, 9.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 4 recordss, 10.7ms\n",
      "Speed: 2.4ms preprocess, 10.7ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 4 recordss, 10.6ms\n",
      "Speed: 2.6ms preprocess, 10.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 recordss, 9.7ms\n",
      "Speed: 2.6ms preprocess, 9.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 4 recordss, 10.5ms\n",
      "Speed: 2.6ms preprocess, 10.5ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 4 recordss, 10.5ms\n",
      "Speed: 2.8ms preprocess, 10.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 recordss, 9.9ms\n",
      "Speed: 2.5ms preprocess, 9.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 recordss, 9.8ms\n",
      "Speed: 2.6ms preprocess, 9.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 recordss, 9.8ms\n",
      "Speed: 2.7ms preprocess, 9.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 recordss, 9.6ms\n",
      "Speed: 2.4ms preprocess, 9.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 recordss, 10.0ms\n",
      "Speed: 2.7ms preprocess, 10.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 recordss, 9.9ms\n",
      "Speed: 2.6ms preprocess, 9.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 recordss, 9.8ms\n",
      "Speed: 2.7ms preprocess, 9.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 recordss, 9.1ms\n",
      "Speed: 2.5ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 recordss, 9.0ms\n",
      "Speed: 2.3ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 recordss, 8.8ms\n",
      "Speed: 2.3ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 4 recordss, 10.3ms\n",
      "Speed: 2.5ms preprocess, 10.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 4 recordss, 9.0ms\n",
      "Speed: 2.3ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images: 100%|██████████| 30/30 [01:58<00:00,  3.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total OCR entries: 120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/12 [00:00<?, ?batch/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch from index 0 to 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   8%|▊         | 1/12 [05:42<1:02:46, 342.44s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch from index 10 to 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  17%|█▋        | 2/12 [11:32<57:50, 347.02s/batch]  Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch from index 20 to 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  25%|██▌       | 3/12 [17:12<51:35, 343.91s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch from index 30 to 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  33%|███▎      | 4/12 [22:41<45:02, 337.78s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch from index 40 to 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  42%|████▏     | 5/12 [28:26<39:42, 340.39s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch from index 50 to 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  50%|█████     | 6/12 [33:57<33:43, 337.19s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch from index 60 to 70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  58%|█████▊    | 7/12 [39:35<28:08, 337.66s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch from index 70 to 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  67%|██████▋   | 8/12 [45:18<22:36, 339.23s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch from index 80 to 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  75%|███████▌  | 9/12 [50:54<16:54, 338.21s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch from index 90 to 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  83%|████████▎ | 10/12 [56:39<11:20, 340.20s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch from index 100 to 110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  92%|█████████▏| 11/12 [1:02:22<05:41, 341.10s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch from index 110 to 120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 12/12 [1:07:53<00:00, 339.44s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Execution Completed!!!!!\n",
      "It took 4195.998757839203 seconds to execute\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# input_folder = folder_to_process_path\n",
    "# file_name = output_txt_path + \".txt\"\n",
    "input_folder=\"/root/NA0219/NA0219_Test Images\"\n",
    "file_name=\"NA0219_test_images.txt\"\n",
    "results = process_image_folder(input_folder, max_workers=8)\n",
    "image_with_ocr = get_image_ocr_list(ocrs)\n",
    "outputs = process_ocr_data(image_with_ocr, ft_model, tokenizer, batch_size=10)\n",
    "final_results = merge_lists(outputs, ocrs)\n",
    "llm_json_output = extract_json_outputs_parallel(final_results, max_workers=8)\n",
    "orig_duplicate = duplicate_keys(llm_json_output)\n",
    "p = split_bbox_info(bounding_boxes_info)\n",
    "filtered_ocr = extract_filtered_info_v2(ocrs, split_info)\n",
    "processed_data = process_json_outputs(orig_duplicate, filtered_ocr, input_folder)\n",
    "cleaned_data = clean_data(processed_data,split_info)\n",
    "sorted_data = group_events_by_image(cleaned_data)\n",
    "organized_data = organize_data(sorted_data)\n",
    "updated_list = update_image_names(organized_data)\n",
    "write_to_pipe_delimited_file(updated_list, file_name)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"\\nExecution Completed!!!!!\")\n",
    "print(f\"It took {elapsed_time} seconds to execute\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66d5d371-4fd1-4df0-873b-df383a325f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import os\n",
    "# import cv2\n",
    "# model = YOLO(\"/root/NA0219/last_5-8-24.pt\")\n",
    "# def detect_records(input_folder):\n",
    "#     # Iterate over all images in the input folder\n",
    "#     for filename in os.listdir(input_folder):\n",
    "#         if filename.endswith(('.jpg', '.jpeg', '.png')):\n",
    "#             img_path = os.path.join(input_folder, filename)\n",
    "#             img = cv2.imread(img_path)\n",
    "#             results = model(img)\n",
    "#             for result in results:\n",
    "#                 if result.boxes.data.shape[0] > 0:  # Check for detections\n",
    "#                     for i, box in enumerate(result.boxes.data.tolist()):\n",
    "#                         xmin, ymin, xmax, ymax, conf, cls = box\n",
    "\n",
    "#                         # Draw the bounding box on the image\n",
    "#                         cv2.rectangle(img, (int(xmin), int(ymin)), (int(xmax), int(ymax)), (0, 255, 0), 5)\n",
    "\n",
    "#             plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "#             plt.title(f\"Detections on {filename}\")\n",
    "#             plt.axis('off')\n",
    "#             plt.show()\n",
    "\n",
    "# input_folder = \"/root/NA0219/NA0219_test_images\"\n",
    "# detect_records(input_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c75dc3b5-4300-45cf-ba43-368bbe7ab7b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(llm_json_output)/len(final_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
